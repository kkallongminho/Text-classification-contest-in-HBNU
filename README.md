
â¸»

ğŸ§  LLM-based Text Classification with LLaMA-3-8B and LoRA

This project performs binary text classification (real vs. AI-generated) using the Meta-Llama-3-8B-Instruct model. It focuses on efficient preprocessing, lightweight fine-tuning with LoRA, and final probability-based submission using softmax.

â¸»

ğŸš€ Features

âœ… 1. Token-Based Preprocessing
	â€¢	Splits text data into 128-token chunks
	â€¢	Discards samples shorter than 64 tokens
	â€¢	Maintains consistent input length for model stability

âœ… 2. Undersampling for Class Balance
	â€¢	Reduces the size of overrepresented class
	â€¢	Ensures balanced training data distribution

âœ… 3. Training with LoRA
	â€¢	Uses Low-Rank Adaptation (LoRA) to fine-tune large models with reduced memory and computation cost
	â€¢	Trains the LLaMA-3-8B model in float32 precision
	â€¢	Utilizes CrossEntropyLoss with a warm-up learning rate scheduler

âœ… 4. Inference with Softmax
	â€¢	Computes softmax probabilities from logits
	â€¢	Outputs the final .npy and .csv files for submission

â¸»

ğŸ› ï¸ Requirements

pip install torch transformers peft accelerate

ğŸ§ª Usage Flow

split_by_128tokens.py 
â†’ count_amount_each_labels.py 
â†’ undersampling.py 
â†’ train_and_save_softmax_llama8b.py 
â†’ save_submission.py

ğŸ“Š Tuning Tips
	â€¢	Increase batch_size if your GPU memory allows
	â€¢	Adjust accumulation_steps, learning_rate, and num_epochs for better performance

â¸»

ğŸ“Œ Notes
	â€¢	Hugging Face access token is required to load the LLaMA-3-8B model
	â€¢	Small batch sizes are used to prevent out-of-memory errors
	â€¢	LoRA enables efficient training of large-scale models even on limited hardware



![ê²°ê³¼ ì´ë¯¸ì§€](á„‰á…³á„á…³á„…á…µá†«á„‰á…£á†º 2025-06-11 á„‹á…©á„’á…® 5.14.37)
