ðŸ§  LLM-based Text Classification with Token Splitting, LoRA, and Softmax Ensemble

This project performs binary text classification (real vs. AI-generated) using transformer-based models like LLaMA 3.2-1B, LLaMA 3.1-3B, and Mistral-7B. It includes:
	â€¢	Efficient data tokenization/splitting
	â€¢	Class balancing via undersampling
	â€¢	Training with LoRA (Low-Rank Adaptation)
	â€¢	Softmax-based ensemble for final submission.

ðŸš€ Features

âœ… 1. Token-Based Splitting
	â€¢	Texts are split into chunks of 128 tokens
	â€¢	Discards samples shorter than 64 tokens
	â€¢	Ensures uniform input size for transformer models

âœ… 2. Undersampling
	â€¢	Balances the dataset by reducing overrepresented class
	â€¢	Shuffles after sampling to improve learning stability

âœ… 3. Training with LoRA
	â€¢	Applies Low-Rank Adaptation (LoRA) to reduce training cost
	â€¢	Trains LLaMA and Mistral models in float32
	â€¢	Uses CrossEntropyLoss with warmup scheduler

âœ… 4. Inference & Ensemble
	â€¢	Applies softmax to each modelâ€™s logits
	â€¢	Averages multiple modelsâ€™ outputs for robust final prediction
	â€¢	Saves .npy and .csv outputs


ðŸ› ï¸ Requirements

pip install torch transformers datasets peft accelerate

ðŸ§ª Usage

1. Data Preprocessing

Split texts into 128-token segments and save them:

2. Train All Models

3. Softmax Ensemble + Submission

split_by_128tokens.py --> count_amount_each_labels.py --> undersampling.py --> train_and_saving_softmax_each_models.py --> ensemble.py



ðŸ“Š Performance Tuning Tips
	â€¢	Use larger batch sizes if memory allows
	â€¢	Tune accumulation_steps, lr, and num_epochs
	â€¢	Optionally experiment with BCEWithLogitsLoss for binary outputs


ðŸ“Œ Notes
	â€¢	All models use Hugging Face access tokens for loading private models
	â€¢	DataLoader uses small batch_size to avoid OOM issues on Colab
	â€¢	LoRA significantly reduces memory usage for large models
